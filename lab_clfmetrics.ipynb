{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3", 
      "name": "python3", 
      "language": "python"
    }, 
    "language_info": {
      "mimetype": "text/x-python", 
      "nbconvert_exporter": "python", 
      "name": "python", 
      "file_extension": ".py", 
      "version": "3.6.8", 
      "pygments_lexer": "ipython3", 
      "codemirror_mode": {
        "version": 3, 
        "name": "ipython"
      }
    }
  }, 
  "nbformat": 4, 
  "nbformat_minor": 2, 
  "cells": [
    {
      "source": [
        "# Classification, Probabilities, and the Confusion Matrix"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 1, 
      "cell_type": "code", 
      "source": [
        "%matplotlib inline\n", 
        "import numpy as np\n", 
        "import scipy as sp\n", 
        "import matplotlib as mpl\n", 
        "import matplotlib.cm as cm\n", 
        "import matplotlib.pyplot as plt\n", 
        "import pandas as pd\n", 
        "import seaborn.apionly as sns"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "source": [
        "We are going to encapsulate some code into handy-dandy functions that we can use for easier model training using cross-validation"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 2, 
      "cell_type": "code", 
      "source": [
        "from sklearn.model_selection import GridSearchCV\n", 
        "from sklearn.model_selection import train_test_split\n", 
        "from sklearn.metrics import confusion_matrix\n", 
        "def cv_optimize(clf, parameters, X, y, n_jobs=1, n_folds=5, score_func=None):\n", 
        "    if score_func:\n", 
        "        gs = GridSearchCV(clf, param_grid=parameters, cv=n_folds, n_jobs=n_jobs, scoring=score_func)\n", 
        "    else:\n", 
        "        gs = GridSearchCV(clf, param_grid=parameters, n_jobs=n_jobs, cv=n_folds)\n", 
        "    gs.fit(X, y)\n", 
        "    print(\"BEST\", gs.best_params_, gs.best_score_)\n", 
        "    best = gs.best_estimator_\n", 
        "    return best\n", 
        "def do_classify(clf, parameters, indf, featurenames, targetname, target1val,mode=\"mask\", reuse_split=None, score_func=None, n_folds=5, n_jobs=1):\n", 
        "    \"\"\"\n", 
        "    Classification made simple (or is it more complex?)\n", 
        "    THIS WORKS FOR 2 Class Classification problems only\n", 
        "    parameters: parameter grid in the sklearn style\n", 
        "    indf: dataframe you feed in\n", 
        "    featurenames: list of columnames corresponding to features you want in your model\n", 
        "    targetname: the column you want to use as target\n", 
        "    target1val: the value of the \"targetname\" column\n", 
        "    mode: mask or split. mask a boolean mask to choose train/test or\n", 
        "        split a dictionary with keys Xtrain/Xtest/ytrain/ytest and values existing\n", 
        "        training and test sets in the canonical form\n", 
        "    reuse_split: the actual mask above or the actuall ditionary, depending upon which\n", 
        "        modu you chose\n", 
        "    score_func: this is from GridSearchCV\n", 
        "    n_folds: cross val folds\n", 
        "    n_jobs: mumber of processes to use in cross-validation\n", 
        "    \n", 
        "    We return classifier, and the train and test sets. We print accuracies\n", 
        "    and the confusion matrix\n", 
        "    \"\"\"\n", 
        "    subdf=indf[featurenames]\n", 
        "    X=subdf.values\n", 
        "    y=(indf[targetname].values==target1val)*1\n", 
        "    if mode==\"mask\":\n", 
        "        print(\"using mask\")\n", 
        "        mask=reuse_split\n", 
        "        Xtrain, Xtest, ytrain, ytest = X[mask], X[~mask], y[mask], y[~mask]\n", 
        "    else:\n", 
        "        print(\"using reuse split\")\n", 
        "        Xtrain, Xtest, ytrain, ytest = reuse_split['Xtrain'], reuse_split['Xtest'], reuse_split['ytrain'], reuse_split['ytest']\n", 
        "    if parameters:\n", 
        "        clf = cv_optimize(clf, parameters, Xtrain, ytrain, n_jobs=n_jobs, n_folds=n_folds, score_func=score_func)\n", 
        "    clf=clf.fit(Xtrain, ytrain)\n", 
        "    training_accuracy = clf.score(Xtrain, ytrain)\n", 
        "    test_accuracy = clf.score(Xtest, ytest)\n", 
        "    print(\"############# based on standard predict ################\")\n", 
        "    print(\"Accuracy on training data: %0.2f\" % (training_accuracy))\n", 
        "    print(\"Accuracy on test data:     %0.2f\" % (test_accuracy))\n", 
        "    print(confusion_matrix(ytest, clf.predict(Xtest)))\n", 
        "    print(\"########################################################\")\n", 
        "    return clf, Xtrain, ytrain, Xtest, ytest"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "execution_count": 3, 
      "cell_type": "code", 
      "source": [
        "from matplotlib.colors import ListedColormap\n", 
        "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n", 
        "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n", 
        "cm = plt.cm.RdBu\n", 
        "cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n", 
        "\n", 
        "def points_plot(ax, Xtr, Xte, ytr, yte, clf, mesh=True, colorscale=cmap_light, cdiscrete=cmap_bold, alpha=0.3, psize=10, zfunc=False):\n", 
        "    h = .02\n", 
        "    X=np.concatenate((Xtr, Xte))\n", 
        "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n", 
        "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n", 
        "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n", 
        "                         np.linspace(y_min, y_max, 100))\n", 
        "\n", 
        "    #plt.figure(figsize=(10,6))\n", 
        "    if mesh:\n", 
        "        if zfunc:\n", 
        "            p0 = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 0]\n", 
        "            p1 = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n", 
        "            Z=zfunc(p0, p1)\n", 
        "        else:\n", 
        "            Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n", 
        "        Z = Z.reshape(xx.shape)\n", 
        "        plt.pcolormesh(xx, yy, Z, cmap=cmap_light, alpha=alpha, axes=ax)\n", 
        "    ax.scatter(Xtr[:, 0], Xtr[:, 1], c=ytr-1, cmap=cmap_bold, s=psize, alpha=alpha,edgecolor=\"k\")\n", 
        "    # and testing points\n", 
        "    yact=clf.predict(Xte)\n", 
        "    ax.scatter(Xte[:, 0], Xte[:, 1], c=yte-1, cmap=cmap_bold, alpha=alpha, marker=\"s\", s=psize+10)\n", 
        "    ax.set_xlim(xx.min(), xx.max())\n", 
        "    ax.set_ylim(yy.min(), yy.max())\n", 
        "    return ax,xx,yy"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "execution_count": 4, 
      "cell_type": "code", 
      "source": [
        "def points_plot_prob(ax, Xtr, Xte, ytr, yte, clf, colorscale=cmap_light, cdiscrete=cmap_bold, ccolor=cm, psize=10, alpha=0.1, prob=True):\n", 
        "    ax,xx,yy = points_plot(ax, Xtr, Xte, ytr, yte, clf, mesh=False, colorscale=colorscale, cdiscrete=cdiscrete, psize=psize, alpha=alpha) \n", 
        "    if prob:\n", 
        "        Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n", 
        "    else:\n", 
        "        Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n", 
        "    Z = Z.reshape(xx.shape)\n", 
        "    plt.contourf(xx, yy, Z, cmap=ccolor, alpha=.2, axes=ax)\n", 
        "    cs2 = plt.contour(xx, yy, Z, cmap=ccolor, alpha=.6, axes=ax)\n", 
        "    plt.clabel(cs2, fmt = '%2.1f', colors = 'k', fontsize=14)\n", 
        "    return ax "
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "source": [
        "## Setting up the data\n", 
        "\n", 
        "(I encountered this dataset in Conway, Drew, and John White. Machine learning for hackers. \" O'Reilly Media, Inc.\", 2012.)"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 5, 
      "cell_type": "code", 
      "source": [
        "dfhw=pd.read_csv(\"data/01_heights_weights_genders.csv\")\n", 
        "dfhw.head()"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "We sample 500 points from 10,000, since we actually want to see trends clearly on the plots"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 6, 
      "cell_type": "code", 
      "source": [
        "df=dfhw.sample(500, replace=False)\n", 
        "np.sum(df.Gender==\"Male\")"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "We split the data into training and test sets...and setup a mask so we can reuse these splits later"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 8, 
      "cell_type": "code", 
      "source": [
        "itrain, itest = train_test_split(range(df.shape[0]), train_size=0.6)\n", 
        "mask=np.ones(df.shape[0], dtype='int')\n", 
        "mask[itrain]=1\n", 
        "mask[itest]=0\n", 
        "mask = (mask==1)\n", 
        "mask[:10]"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "## Logistic regression\n", 
        "\n", 
        "\n", 
        "$$\n", 
        "\\renewcommand{\\like}{{\\cal L}}\n", 
        "\\renewcommand{\\loglike}{{\\ell}}\n", 
        "\\renewcommand{\\err}{{\\cal E}}\n", 
        "\\renewcommand{\\dat}{{\\cal D}}\n", 
        "\\renewcommand{\\hyp}{{\\cal H}}\n", 
        "\\renewcommand{\\Ex}[2]{E_{#1}[#2]}\n", 
        "\\renewcommand{\\x}{{\\mathbf x}}\n", 
        "\\renewcommand{\\v}[1]{{\\mathbf #1}}\n", 
        "$$\n", 
        "\n", 
        "\n", 
        "Previously, we saw the loss for Logistic regression and noted that it is a loss for probability estimation...and not a loss for making decisions. We'll go into these dual losses soon..\n", 
        "\n", 
        "$$R_{\\cal{D}}(h(x)) = -\\loglike = -log \\like = - log(P(y|\\v{x},\\v{w})).$$\n", 
        "\n", 
        "\n", 
        "Thus\n", 
        "\n", 
        "\\begin{eqnarray*}\n", 
        "R_{\\cal{D}}(h(x)) &=& -log\\left(\\prod_{y_i \\in \\cal{D}} h(\\v{w}\\cdot\\v{x_i})^{y_i} \\left(1 - h(\\v{w}\\cdot\\v{x_i}) \\right)^{(1-y_i)}\\right)\\\\\n", 
        "                  &=& -\\sum_{y_i \\in \\cal{D}} log\\left(h(\\v{w}\\cdot\\v{x_i})^{y_i} \\left(1 - h(\\v{w}\\cdot\\v{x_i}) \\right)^{(1-y_i)}\\right)\\\\                  \n", 
        "                  &=& -\\sum_{y_i \\in \\cal{D}} log\\,h(\\v{w}\\cdot\\v{x_i})^{y_i} + log\\,\\left(1 - h(\\v{w}\\cdot\\v{x_i}) \\right)^{(1-y_i)}\\\\\n", 
        "                  &=& - \\sum_{y_i \\in \\cal{D}} \\left ( y_i log(h(\\v{w}\\cdot\\v{x})) + ( 1 - y_i) log(1 - h(\\v{w}\\cdot\\v{x})) \\right )\n", 
        "\\end{eqnarray*}\n", 
        "\n", 
        "where\n", 
        "\n", 
        "$$h(z) = \\frac{1}{1 + e^{-z}}.$$\n", 
        "\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Notice that its L2 regularized.... by default"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 9, 
      "cell_type": "code", 
      "source": [
        "from sklearn.linear_model import LogisticRegression\n", 
        "parameters = {\"C\": [0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]}\n", 
        "clflog = LogisticRegression(solver='lbfgs')\n", 
        "clflog, Xtrain, ytrain, Xtest, ytest=do_classify(clflog, parameters, df, ['Height','Weight'],'Gender', \"Male\", mode=\"mask\", reuse_split=mask)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "In `sklearn`, `clf.predict(test_data)` makes predictions on the assumption that a 0.5 probability threshold is the appropriate thing to do. Make predictions on the test set"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 11, 
      "cell_type": "code", 
      "source": [
        "# your code here\n"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "In `sklearn`, `predict_proba` gives us the probabilities. Find the probabilities on the test set."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 12, 
      "cell_type": "code", 
      "source": [
        "clflog.predict_proba(Xtest)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "What do these probabilities correspond to? The second column (`[:,1]` in numpy parlance, google numpy indexing to understand the syntax) gives the probability that the sample is a 1 (or +ive), here Male.\n", 
        "\n", 
        "Make a histogram of these probabilities. Interpret them."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 27, 
      "cell_type": "code", 
      "source": [
        "# your code here\n"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Lots of sure females and sure males when you plot the probability of being a male. \n", 
        "\n", 
        "At this point you might want to see how this histogram looks in the 2 dimensional space of our predictors."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 13, 
      "cell_type": "code", 
      "source": [
        "Xtr=np.concatenate((Xtrain, Xtest))\n", 
        "plt.figure()\n", 
        "ax=plt.gca()\n", 
        "with sns.plotting_context('poster'):\n", 
        "    points_plot(ax, Xtrain, Xtest, ytrain, ytest, clflog);"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "We can plot the probability contours: these are rather tight!"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 14, 
      "cell_type": "code", 
      "source": [
        "plt.figure()\n", 
        "ax=plt.gca()\n", 
        "points_plot(ax, Xtrain, Xtest, ytrain, ytest, clflog, mesh=False, alpha=0.001);\n", 
        "points_plot_prob(ax, Xtrain, Xtest, ytrain, ytest, clflog);"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "The score function of the estimator is used to evaluate a parameter setting. These are the sklearn.metrics.accuracy_score for classification and sklearn.metrics.r2_score for regression. For some applications, other scoring functions are better suited (for example in unbalanced classification, the accuracy score is often uninformative). We can pass other scorers to `GridSearchCV`."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 29, 
      "cell_type": "code", 
      "source": [
        "clflog.score(Xtest, ytest)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "### Callibration\n", 
        "\n", 
        "(from description by Chris Beaumont in CS109)\n", 
        "\n", 
        "Probabilistic models  have the nice property that they compute probabilities of a particular classification -- the predict_proba and predict_log_proba methods  compute these probabilities.\n", 
        "\n", 
        "You should always assess whether these probabilities are calibrated -- that is, whether a prediction made with a confidence of x% is correct approximately x% of the time.\n", 
        "\n", 
        "Let's make a plot to assess model calibration. Schematically, we want something like this:\n", 
        "\n", 
        "![](http://i.imgur.com/ea5B6zr.png)"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 33, 
      "cell_type": "code", 
      "source": [
        "from sklearn.metrics import brier_score_loss\n", 
        "from sklearn.calibration import  calibration_curve\n", 
        "\n", 
        "prob_pos = clflog.predict_proba(Xtest)[:, 1]\n", 
        "fraction_of_positives, mean_predicted_value = \\\n", 
        "            calibration_curve(ytest, prob_pos, n_bins=10)\n", 
        "clf_score = brier_score_loss(ytest, prob_pos, pos_label=ytest.max())"
      ], 
      "outputs": [], 
      "metadata": {}
    }, 
    {
      "execution_count": 34, 
      "cell_type": "code", 
      "source": [
        "fraction_of_positives, mean_predicted_value"
      ], 
      "outputs": [], 
      "metadata": {}
    }, 
    {
      "execution_count": 35, 
      "cell_type": "code", 
      "source": [
        "fig = plt.figure(figsize=(10, 10))\n", 
        "ax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2)\n", 
        "ax2 = plt.subplot2grid((3, 1), (2, 0))\n", 
        "ax1.plot(mean_predicted_value, fraction_of_positives, \"s-\",\n", 
        "         label=\"(%1.3f)\" % (clf_score))\n", 
        "ax1.plot([0,1], [0,1], \"k--\")\n", 
        "ax2.hist(prob_pos, range=(0, 1), bins=10, label=\"logistic\",\n", 
        "         histtype=\"step\", lw=2)\n", 
        "\n", 
        "ax1.set_ylabel(\"Fraction of positives\")\n", 
        "ax1.set_ylim([-0.05, 1.05])\n", 
        "ax1.legend(loc=\"lower right\")\n", 
        "ax1.set_title('Calibration plots  (reliability curve)')\n", 
        "\n", 
        "ax2.set_xlabel(\"Mean predicted value\")\n", 
        "ax2.set_ylabel(\"Count\")\n", 
        "ax2.legend(loc=\"upper center\", ncol=2)\n", 
        "\n", 
        "plt.tight_layout()"
      ], 
      "outputs": [], 
      "metadata": {}
    }, 
    {
      "source": [
        "## The confusion Matrix"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        " A classifier will get some samples right, and some wrong. Generally we see which ones it gets right and which ones it gets wrong on the test set. There,\n", 
        "\n", 
        "- the samples that are +ive and the classifier predicts as +ive are called True Positives (TP)\n", 
        "- the samples that are -ive and the classifier predicts (wrongly) as +ive are called False Positives (FP)\n", 
        "- the samples that are -ive and the classifier predicts as -ive are called True Negatives (TN)\n", 
        "- the samples that are +ive and the classifier predicts as -ive are called False Negatives (FN)\n", 
        "\n", 
        "A classifier produces a confusion matrix from these which lookslike this:\n", 
        "\n", 
        "![hwimages](./images/confusionmatrix.png)\n", 
        "\n", 
        "\n", 
        "IMPORTANT NOTE: In sklearn, to obtain the confusion matrix in the form above, always have the observed `y` first, i.e.: use as `confusion_matrix(y_true, y_pred)`"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 31, 
      "cell_type": "code", 
      "source": [
        "confusion_matrix(ytest, clflog.predict(Xtest))"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Given these definitions, we typically calculate a few metrics for our classifier. First, the **True Positive Rate**:\n", 
        "\n", 
        "$$TPR = Recall = \\frac{TP}{OP} = \\frac{TP}{TP+FN},$$\n", 
        "\n", 
        "also called the Hit Rate: the fraction of observed positives (1s) the classifier gets right, or how many true positives were recalled. Maximizing the recall towards 1 means keeping down the false negative rate. In a classifier try to find cancer patients, this is the number we want to maximize.\n", 
        "\n", 
        "The **False Positive Rate** is defined as\n", 
        "\n", 
        "$$FPR = \\frac{FP}{ON} = \\frac{FP}{FP+TN},$$\n", 
        "\n", 
        "also called the False Alarm Rate, the fraction of observed negatives (0s) the classifier gets wrong. In general, you want this number to be low. Instead, you might want to maximize the\n", 
        "**Precision**,which tells you how many of the predicted positive(1) hits were truly positive\n", 
        "\n", 
        "$$Precision = \\frac{TP}{PP} = \\frac{TP}{TP+FP}.$$\n", 
        "\n", 
        "Finally the **F1** score gives us the Harmonic Score of Precision and Recall. Many analysts will try and find a classifier that maximizes this score, since it tries to minimize both false positives and false negatives simultaneously, and is thus a bit more precise in what it is trying to do than the accuracy.\n", 
        "\n", 
        "$$F1 =  \\frac{2*Recall*Precision}{Recall + Precision}$$\n", 
        "\n", 
        "However, in a case like that of a cancer classifier, we will wish to minimize false nagatives at the expense of false positives: it is ok to send perfectly healthy patients for cancer folloup if that is the price we must pay for not missing any sick ones.\n", 
        "\n", 
        "`scikit-learn` helpfully gives us a classification report with all these numbers"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 32, 
      "cell_type": "code", 
      "source": [
        "from sklearn.metrics import classification_report\n", 
        "print(classification_report(ytest, clflog.predict(Xtest)))"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "#### The cancer doctor\n", 
        "\n", 
        "Do you really want to be setting a threshold of 0.5 probability to predict if a patient has cancer or not? The false negative problem: ie the chance you predict someone dosent have cancer who has cancer is much higher for such a threshold. You could kill someone by telling them not to get a biopsy. Why not play it safe and assume a much lower threshold: for eg, if the probability of 1(cancer) is greater than 0.05, we'll call it a 1.\n", 
        "\n", 
        "Write a function `t_repredict(est,t, Xtest)` which takes your classifier, a probability threshold, and a  features set in the canonical form, and returns a set of predictions."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 15, 
      "cell_type": "code", 
      "source": [
        "# your code here\n"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "source": [
        "Print the confusion matrix to see how the false negatives get suppressed?"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 16, 
      "cell_type": "code", 
      "source": [
        "# your code here\n"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "## ROC Curve\n", 
        "\n", 
        "The images in this section are from Provost, Foster; Fawcett, Tom (2013-07-27). Data Science for Business: What you need to know about data mining and data-analytic thinking  O'Reilly Media. Great book!\n", 
        "\n", 
        "We can check on the thresholds we talked about in the previous section and compare our classifiers to each other and the baseline models using the ROC curves you learned about in class. \n", 
        "\n", 
        "Remember that ROC curves are actually a set of classifiers, in which we move the threshold for classifying a sample as positive from 1 to 0. Each point on a ROC curve is a separate classifier obtained by considering a different threshold. (In the standard scenario, where we used the  classifier accuracy, this threshold is implicitly set at 0.5, and we have only one classifier).\n", 
        "\n", 
        "![m:roc curve](images/roc-curve.png)\n", 
        "\n", 
        "The way ROC curves are calulated is this. We start with a large threshold, something like 0.99 or so. This means that only samples with a probability of being positive higher than that threshold are classified as positive. That is the really really really positive ones! The idea then is to decrease this threshold, such that more and more samples get classified as positive.\n", 
        "\n", 
        "![howto roc](images/howtoroc.png)\n", 
        "\n", 
        "The practical way to do this is to order the samples by probability of being positive, or in the case of the SVM, by the `decision_function` or distance from the separating hyperplane. Then consider the sample with the highest score or highest probability of being positive. At first, only this sample is positive. Then, we take the sample with the next highest score, and call it positive. As we go down the list, we go down a threshold in score or probability. \n", 
        "\n", 
        "Now, for each such situation: only 1 positive, now 2 positive,....you can imagine a different classifier with a different confusion matrix. It will have its own false positives, tre positives, etc. Its actually the same original classifier, but with a different threshold each time.\n", 
        "\n", 
        "As we keep going down the list, decreasing the threshold, more and more samples become positive, and at first, the true positives rise faster than the false positives. Once past a certain point, false positives increase faster than true positives. Now, if you want a balanced classifier, you look at this turn-around point...the northwest corner, so to speak. But if you want a classifier which penalizes false positives and false negatives differently, the point you want is different.\n", 
        "\n", 
        "Here is the confusion matrix again:\n", 
        "\n", 
        "![hwimages](./images/confusionmatrix.png)\n", 
        "\n", 
        "\n", 
        "To make a ROC curve you plot the True Positive Rate, \n", 
        "\n", 
        "$$TPR=\\frac{TP}{OP}$$\n", 
        "\n", 
        "against the False Positive Rate,\n", 
        "\n", 
        "$$FPR=\\frac{FP}{ON}$$\n", 
        "\n", 
        "as you go through this process of going down the list of samples. ROC curves are useful because they calculate one classifier per threshold and show you where you are in TPR/FPR space without making any assumptions about the utility matrix or which threshold is appropriate.\n", 
        "\n", 
        "Notice that the ROC curve has a very interesting property: if you look at the confusion matrix above, TPR is only calculated from the observed \"1\" row while FPR is calculated from the observed '0' row. This means that the ROC curve is independent of the class balance/imbalance on the test set, and thus works for all ratios of positive to negative samples. The balance picks a point on the curve, as you can read below.\n", 
        "\n", 
        "A rote reading of the ROC curve (go to the \"northwest\" corner) is a bad idea: you must fold in the curve with any assumptions you are making about costs. More on this in the next lab. Still, on the whole, a curve with a greater AUC (area under curve), or further away from the line of randomness, will give us a rough idea of what might be a better classifier.\n", 
        "\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 37, 
      "cell_type": "code", 
      "source": [
        "def make_roc(name, clf, ytest, xtest, ax=None, labe=5, proba=True, skip=0):\n", 
        "    initial=False\n", 
        "    if not ax:\n", 
        "        ax=plt.gca()\n", 
        "        initial=True\n", 
        "    if proba:\n", 
        "        fpr, tpr, thresholds=roc_curve(ytest, clf.predict_proba(xtest)[:,1])\n", 
        "    else:\n", 
        "        fpr, tpr, thresholds=roc_curve(ytest, clf.decision_function(xtest))\n", 
        "    roc_auc = auc(fpr, tpr)\n", 
        "    if skip:\n", 
        "        l=fpr.shape[0]\n", 
        "        ax.plot(fpr[0:l:skip], tpr[0:l:skip], '.-', alpha=0.3, label='ROC curve for %s (area = %0.2f)' % (name, roc_auc))\n", 
        "    else:\n", 
        "        ax.plot(fpr, tpr, '.-', alpha=0.3, label='ROC curve for %s (area = %0.2f)' % (name, roc_auc))\n", 
        "    label_kwargs = {}\n", 
        "    label_kwargs['bbox'] = dict(\n", 
        "        boxstyle='round,pad=0.3', alpha=0.2,\n", 
        "    )\n", 
        "    for k in range(0, fpr.shape[0],labe):\n", 
        "        #from https://gist.github.com/podshumok/c1d1c9394335d86255b8\n", 
        "        threshold = str(np.round(thresholds[k], 2))\n", 
        "        ax.annotate(threshold, (fpr[k], tpr[k]), **label_kwargs)\n", 
        "    if initial:\n", 
        "        ax.plot([0, 1], [0, 1], 'k--')\n", 
        "        ax.set_xlim([0.0, 1.0])\n", 
        "        ax.set_ylim([0.0, 1.05])\n", 
        "        ax.set_xlabel('False Positive Rate')\n", 
        "        ax.set_ylabel('True Positive Rate')\n", 
        "        ax.set_title('ROC')\n", 
        "    ax.legend(loc=\"lower right\")\n", 
        "    return ax"
      ], 
      "outputs": [], 
      "metadata": {}
    }, 
    {
      "execution_count": 38, 
      "cell_type": "code", 
      "source": [
        "with sns.plotting_context('poster'):\n", 
        "    from sklearn.metrics import roc_curve, auc\n", 
        "    ax=make_roc(\"logistic\", clflog, ytest, Xtest, labe=6)"
      ], 
      "outputs": [], 
      "metadata": {}
    }, 
    {
      "source": [
        "## Decision Making\n", 
        "\n", 
        "So far we have just been reporting accuracies. But as in the cancer doctor case, many a time we'll actually want to use a classifier to make decisions.\n", 
        "\n", 
        "The accuracy itself is NOT the loss that went into estimating the parameters of the model in the logistic regression. We are only using it as an **estimation loss** or a **evaluation metric**. \n", 
        "\n", 
        "The loss correponding to the accuracy metric is:\n", 
        "\n", 
        "The 1-0 loss:\n", 
        "\n", 
        "$$l = \\mathbf{1}_{h \\ne y}.$$\n", 
        "\n", 
        "where $h$ is the classification **decision** we make (for regression we used $l = (h-y)^2$). The symbol $\\mathbf{1}$ means that if $h$ is not equal to the \"true\" value of the point $y$, penalize by 1. Then the risk is:\n", 
        "\n", 
        "$$ R_{\\cal{D}}(h(x)) = \\frac{1}{N} \\sum_{y_i \\in \\cal{D}} l = \\frac{1}{N} \\sum_{y_i \\in \\cal{D}} \\mathbf{1}_{h \\ne y_i} $$\n", 
        "\n", 
        "Thus if 5 out of 50 samples are misclassified, then the risk is 0.1. This of course means that 90% of the samples are correctly classified. This number is called the **accuracy score** or **utility**:\n", 
        "\n", 
        "$$ U_{\\cal{D}}(h(x))  = \\frac{1}{N} \\sum_{y_i \\in \\cal{D}} \\mathbf{1}_{h = y_i} $$\n", 
        "\n", 
        "`sklearn` allows us to use another scorer by choosing an appropriate scoring function in our cross-validation metric."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }
  ]
}